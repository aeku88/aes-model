{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-19T19:21:10.910420Z",
     "start_time": "2025-02-19T19:21:09.507230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, load_dataset\n",
    "\n",
    "data_files = {\"train\": \"asap-aes-processed/training_set_segmented - prompt_1.csv\"}\n",
    "\n",
    "asap_ds = load_dataset(\"csv\", data_files=data_files)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:21:10.942539Z",
     "start_time": "2025-02-19T19:21:10.922553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lowercase_essay(example):\n",
    "    return {'essay': example['essay'].lower()}\n",
    "\n",
    "asap_ds = asap_ds.map(lowercase_essay)"
   ],
   "id": "54f816ea8c2ba3d1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:21:16.286186Z",
     "start_time": "2025-02-19T19:21:11.105968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertModel, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"essay\"], truncation=True, max_length=512)"
   ],
   "id": "39f14a05f5be55ed",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:21:16.336727Z",
     "start_time": "2025-02-19T19:21:16.315207Z"
    }
   },
   "cell_type": "code",
   "source": "ds_tokenized = asap_ds.map(tokenize_function, batched=True)",
   "id": "d02f3376a00bcce4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:21:16.382148Z",
     "start_time": "2025-02-19T19:21:16.357339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "id": "cbe886cf960f4b26",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:57:20.617471Z",
     "start_time": "2025-02-21T15:57:18.522205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RelativePosition(nn.Module):\n",
    "\n",
    "    def __init__(self, num_units, max_relative_position):\n",
    "        super().__init__()\n",
    "        self.num_units = num_units\n",
    "        self.max_relative_position = max_relative_position\n",
    "        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n",
    "        nn.init.xavier_uniform_(self.embeddings_table)\n",
    "\n",
    "    def forward(self, length_q, length_k):\n",
    "        range_vec_q = torch.arange(length_q)\n",
    "        range_vec_k = torch.arange(length_k)\n",
    "        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n",
    "        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n",
    "        final_mat = distance_mat_clipped + self.max_relative_position\n",
    "        final_mat = torch.LongTensor(final_mat).cuda()\n",
    "        embeddings = self.embeddings_table[final_mat].cuda()\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.max_relative_position = 2\n",
    "\n",
    "        self.relative_position_k = RelativePosition(self.head_dim, self.max_relative_position)\n",
    "        self.relative_position_v = RelativePosition(self.head_dim, self.max_relative_position)\n",
    "\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        batch_size = query.shape[0]\n",
    "        len_k = key.shape[1]\n",
    "        len_q = query.shape[1]\n",
    "        len_v = value.shape[1]\n",
    "\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        r_q1 = query.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        r_k1 = key.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2))\n",
    "\n",
    "        r_q2 = query.permute(1, 0, 2).contiguous().view(len_q, batch_size*self.n_heads, self.head_dim)\n",
    "        r_k2 = self.relative_position_k(len_q, len_k)\n",
    "        attn2 = torch.matmul(r_q2, r_k2.transpose(1, 2)).transpose(0, 1)\n",
    "        attn2 = attn2.contiguous().view(batch_size, self.n_heads, len_q, len_k)\n",
    "        attn = (attn1 + attn2) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attn = self.dropout(torch.softmax(attn, dim = -1))\n",
    "\n",
    "        #attn = [batch size, n heads, query len, key len]\n",
    "        r_v1 = value.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        weight1 = torch.matmul(attn, r_v1)\n",
    "        r_v2 = self.relative_position_v(len_q, len_v)\n",
    "        weight2 = attn.permute(2, 0, 1, 3).contiguous().view(len_q, batch_size*self.n_heads, len_k)\n",
    "        weight2 = torch.matmul(weight2, r_v2)\n",
    "        weight2 = weight2.transpose(0, 1).contiguous().view(batch_size, self.n_heads, len_q, self.head_dim)\n",
    "\n",
    "        x = weight1 + weight2\n",
    "\n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "\n",
    "        #x = [batch size, query len, hid dim]\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        #x = [batch size, query len, hid dim]\n",
    "\n",
    "        return x\n"
   ],
   "id": "92ab0e58021dd8d4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "789dc8f19ea141a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T19:21:17.965127Z",
     "start_time": "2025-02-19T19:21:17.754717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "class AESModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AESModel, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-uncased\", config=AutoConfig.from_pretrained(\"bert-base-uncased\", output_attentions=True, output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.response_attn = MultiHeadAttentionLayer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n"
   ],
   "id": "79dde4ce3adab2eb",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
